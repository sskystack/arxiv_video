#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ArXiv Video Downloader - ä¸»å…¥å£æ–‡ä»¶
è‡ªåŠ¨ä¸‹è½½ ArXiv è®ºæ–‡é¡¹ç›®é¡µé¢è§†é¢‘çš„å·¥å…·

ä½¿ç”¨æ–¹å¼:
    python main.py [--workers N] [--download-dir PATH] [--max-papers N] [--field FIELD]

ç¤ºä¾‹:
    python main.py --workers 8 --download-dir ~/Videos/arxiv
    python main.py --max-papers 50 --field cs.CV
"""

import os
import sys
import argparse
from typing import Optional

from utils.logger import setup_logger, get_logger
from core.crawler import ArxivVideoCrawler


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='ArXiv Video Downloader - ä¸‹è½½æœ€æ–°ä¸€å¤©çš„è®ºæ–‡è§†é¢‘',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s --workers 8                           # ä½¿ç”¨8ä¸ªçº¿ç¨‹
  %(prog)s --download-dir ~/Videos/arxiv         # æŒ‡å®šä¸‹è½½ç›®å½•
  %(prog)s --max-papers 50                       # æœ€å¤šä¸‹è½½50ç¯‡è®ºæ–‡çš„è§†é¢‘
  %(prog)s --field cs.AI                         # ä¸‹è½½AIé¢†åŸŸè®ºæ–‡
        """
    )
    
    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=4,
        help='ä¸‹è½½çº¿ç¨‹æ•° (é»˜è®¤: 4, èŒƒå›´: 1-16)'
    )
    
    parser.add_argument(
        '--download-dir', '-d',
        type=str,
        default=os.path.expanduser('~/Movies/arxiv_video'),
        help='è§†é¢‘ä¸‹è½½ç›®å½• (é»˜è®¤: ~/Movies/arxiv_video)'
    )
    
    parser.add_argument(
        '--max-papers', '-m',
        type=int,
        default=1000,
        help='æœ€å¤§è®ºæ–‡æ•°é‡ (é»˜è®¤: 1000)'
    )
    
    parser.add_argument(
        '--field', '-f',
        type=str,
        default='cs.CV',
        help='è®ºæ–‡é¢†åŸŸ (é»˜è®¤: cs.CV)'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    return parser.parse_args()


def validate_arguments(args: argparse.Namespace) -> bool:
    """éªŒè¯å‘½ä»¤è¡Œå‚æ•°"""
    # éªŒè¯çº¿ç¨‹æ•°
    if not 1 <= args.workers <= 16:
        print(f"é”™è¯¯: çº¿ç¨‹æ•°å¿…é¡»åœ¨ 1-16 ä¹‹é—´ï¼Œå½“å‰å€¼: {args.workers}")
        return False
    
    # éªŒè¯æœ€å¤§è®ºæ–‡æ•°
    if args.max_papers <= 0:
        print(f"é”™è¯¯: æœ€å¤§è®ºæ–‡æ•°å¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {args.max_papers}")
        return False
    
    # éªŒè¯ä¸‹è½½ç›®å½•
    try:
        args.download_dir = os.path.abspath(os.path.expanduser(args.download_dir))
        os.makedirs(args.download_dir, exist_ok=True)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åˆ›å»ºä¸‹è½½ç›®å½• {args.download_dir}: {e}")
        return False
    
    return True


def print_summary(results: list, args: argparse.Namespace) -> None:
    """æ‰“å°ä¸‹è½½ç»“æœæ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“Š ä¸‹è½½ç»“æœæ‘˜è¦")
    print("="*60)
    
    total_papers = len(results)
    total_videos = sum(len(result['videos']) for result in results)
    
    print(f"æˆåŠŸå¤„ç†çš„è®ºæ–‡æ•°é‡: {total_papers}")
    print(f"æ€»ä¸‹è½½è§†é¢‘æ•°é‡: {total_videos}")
    
    if results:
        print(f"è§†é¢‘ä¿å­˜ç›®å½•: {args.download_dir}")
        print("\nğŸ“ è¯¦ç»†ä¿¡æ¯:")
        
        for i, result in enumerate(results, 1):
            paper = result['paper']
            videos = result['videos']
            
            print(f"\n{i}. è®ºæ–‡ID: {paper['id']}")
            print(f"   æ ‡é¢˜: {paper['title'][:80]}...")
            print(f"   ä½œè€…: {', '.join(paper['authors'])}")
            if 'submitted_date' in paper:
                print(f"   æäº¤æ—¥æœŸ: {paper['submitted_date']}")
            print(f"   è§†é¢‘æ•°é‡: {len(videos)}")
            
            for j, video in enumerate(videos, 1):
                filename = os.path.basename(video['local_path'])
                print(f"     è§†é¢‘{j}: {filename}")
    else:
        print("\nâŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘")
        print("\nå¯èƒ½çš„åŸå› :")
        print("â€¢ ä»Šå¤©è¯¥é¢†åŸŸæ²¡æœ‰è®ºæ–‡å‘å¸ƒ")
        print("â€¢ è®ºæ–‡æ²¡æœ‰é¡¹ç›®ä¸»é¡µé“¾æ¥")
        print("â€¢ é¡¹ç›®ä¸»é¡µæ²¡æœ‰è§†é¢‘å†…å®¹")
        print("â€¢ ç½‘ç»œè¿æ¥é—®é¢˜")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå’ŒéªŒè¯å‚æ•°
    args = parse_arguments()
    
    if not validate_arguments(args):
        sys.exit(1)
    
    # è®¾ç½®æ—¥å¿—
    log_level = 10 if args.verbose else 20  # DEBUG if verbose else INFO
    logger = setup_logger('arxiv_crawler', log_level)
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("ğŸš€ ArXiv Video Downloader")
    print("="*50)
    print(f"è®ºæ–‡é¢†åŸŸ: {args.field}")
    print(f"æœ€å¤§è®ºæ–‡æ•°: {args.max_papers}")
    print(f"ä¸‹è½½çº¿ç¨‹æ•°: {args.workers}")
    print(f"ä¸‹è½½ç›®å½•: {args.download_dir}")
    print("="*50)
    
    logger.info("ArXiv è§†é¢‘ä¸‹è½½å™¨å¯åŠ¨")
    logger.info(f"é…ç½® - é¢†åŸŸ: {args.field}, çº¿ç¨‹æ•°: {args.workers}, "
               f"æœ€å¤§è®ºæ–‡æ•°: {args.max_papers}, ä¸‹è½½ç›®å½•: {args.download_dir}")
    
    crawler = None
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = ArxivVideoCrawler(
            download_folder=args.download_dir,
            max_workers=args.workers
        )
        
        # å¼€å§‹çˆ¬å–
        results = crawler.crawl_latest_day_videos(
            field=args.field,
            max_papers=args.max_papers
        )
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print_summary(results, args)
        
        logger.info("ArXiv è§†é¢‘ä¸‹è½½å™¨å®Œæˆ")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        logger.info("ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
        sys.exit(1)
        
    finally:
        if crawler:
            crawler.close()


if __name__ == "__main__":
    main()