#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ArXiv Video Downloader - ä¸»å…¥å£æ–‡ä»¶
è‡ªåŠ¨ä¸‹è½½ ArXiv è®ºæ–‡é¡¹ç›®é¡µé¢è§†é¢‘çš„å·¥å…·

ä½¿ç”¨æ–¹å¼:
    python main.py [--workers N] [--download-dir PATH] [--max-papers N] [--field FIELD]

ç¤ºä¾‹:
    python main.py --workers 8 --download-dir ~/Videos/arxiv
    python main.py --max-papers 50 --field cs.CV
"""

import os
import sys
import argparse
from typing import Optional

from utils.logger import setup_logger, get_logger
from core.crawler import ArxivVideoCrawler


def parse_arguments() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='ArXiv Video Downloader - ä¸‹è½½æœ€æ–°ä¸€å¤©çš„è®ºæ–‡è§†é¢‘',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s --workers 8                           # ä½¿ç”¨8ä¸ªçº¿ç¨‹
  %(prog)s --download-dir ~/Videos/arxiv         # æŒ‡å®šä¸‹è½½ç›®å½•
  %(prog)s --max-papers 50                       # æœ€å¤šä¸‹è½½50ç¯‡è®ºæ–‡çš„è§†é¢‘
  %(prog)s --field cs.AI                         # ä¸‹è½½AIé¢†åŸŸè®ºæ–‡
  %(prog)s --publication-date 20250820           # ä¸‹è½½æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡
  %(prog)s --skip-existing                       # è·³è¿‡å·²å­˜åœ¨resè§†é¢‘çš„è®ºæ–‡
  %(prog)s --cookies-from-browser chrome         # ä½¿ç”¨Chromeçš„Cookieä¸‹è½½YouTubeè§†é¢‘
  %(prog)s -p 20250819 -w 8 -m 100 -s            # ç»„åˆä½¿ç”¨å‚æ•°
        """)
    
    parser.add_argument(
        '--workers', '-w',
        type=int,
        default=4,
        help='ä¸‹è½½çº¿ç¨‹æ•° (é»˜è®¤: 4, èŒƒå›´: 1-16)'
    )
    
    parser.add_argument(
        '--download-dir', '-d',
        type=str,
        default=os.path.expanduser('~/Movies/arxiv_video'),
        help='è§†é¢‘ä¸‹è½½ç›®å½• (é»˜è®¤: ~/Movies/arxiv_video)'
    )
    
    parser.add_argument(
        '--max-papers', '-m',
        type=int,
        default=1000,
        help='æœ€å¤§è®ºæ–‡æ•°é‡ (é»˜è®¤: 1000)'
    )
    
    parser.add_argument(
        '--field', '-f',
        type=str,
        default='cs.CV',
        help='è®ºæ–‡é¢†åŸŸ (é»˜è®¤: cs.CV)'
    )
    
    parser.add_argument(
        '--publication-date', '-p',
        type=str,
        default=None,
        help='æŒ‡å®šè®ºæ–‡å‘å¸ƒæ—¥æœŸ (æ ¼å¼: YYYYMMDDï¼Œå¦‚: 20250820)ã€‚å¦‚ä¸æŒ‡å®šï¼Œè‡ªåŠ¨è·å–æœ€æ–°æ—¥æœŸ'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—'
    )
    
    parser.add_argument(
        '--skip-existing', '-s',
        action='store_true',
        default=False,
        help='è·³è¿‡å·²å­˜åœ¨resè§†é¢‘çš„è®ºæ–‡ï¼Œä¸è¿›è¡Œé‡å¤å¤„ç† (é»˜è®¤: False)'
    )

    parser.add_argument(
        '--cookies-from-browser',
        type=str,
        default=None,
        help='æŒ‡å®šæµè§ˆå™¨åç§° (ä¾‹å¦‚: chrome, firefox) ä»¥åŠ è½½cookieï¼Œç”¨äºéœ€è¦ç™»å½•çš„è§†é¢‘ç½‘ç«™ (å¦‚YouTube)'
    )
    
    return parser.parse_args()


def validate_arguments(args: argparse.Namespace) -> bool:
    """éªŒè¯å‘½ä»¤è¡Œå‚æ•°"""
    # éªŒè¯çº¿ç¨‹æ•°
    if not 1 <= args.workers <= 16:
        print(f"é”™è¯¯: çº¿ç¨‹æ•°å¿…é¡»åœ¨ 1-16 ä¹‹é—´ï¼Œå½“å‰å€¼: {args.workers}")
        return False
    
    # éªŒè¯æœ€å¤§è®ºæ–‡æ•°
    if args.max_papers <= 0:
        print(f"é”™è¯¯: æœ€å¤§è®ºæ–‡æ•°å¿…é¡»å¤§äº0ï¼Œå½“å‰å€¼: {args.max_papers}")
        return False
    
    # éªŒè¯å‘å¸ƒæ—¥æœŸæ ¼å¼
    if args.publication_date:
        if not _validate_date_format(args.publication_date):
            print(f"é”™è¯¯: å‘å¸ƒæ—¥æœŸæ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºYYYYMMDDæ ¼å¼ï¼Œå½“å‰å€¼: {args.publication_date}")
            return False
    
    # éªŒè¯ä¸‹è½½ç›®å½•
    try:
        args.download_dir = os.path.abspath(os.path.expanduser(args.download_dir))
        os.makedirs(args.download_dir, exist_ok=True)
    except Exception as e:
        print(f"é”™è¯¯: æ— æ³•åˆ›å»ºä¸‹è½½ç›®å½• {args.download_dir}: {e}")
        return False
    
    return True


def _validate_date_format(date_str: str) -> bool:
    """éªŒè¯æ—¥æœŸæ ¼å¼æ˜¯å¦ä¸ºYYYYMMDD"""
    try:
        if len(date_str) != 8:
            return False
        
        year = int(date_str[:4])
        month = int(date_str[4:6])
        day = int(date_str[6:8])
        
        # åŸºæœ¬èŒƒå›´æ£€æŸ¥
        if not (1900 <= year <= 2100):
            return False
        if not (1 <= month <= 12):
            return False
        if not (1 <= day <= 31):
            return False
        
        # ä½¿ç”¨datetimeè¿›è¡Œæ›´ä¸¥æ ¼çš„éªŒè¯
        from datetime import datetime
        datetime.strptime(date_str, '%Y%m%d')
        return True
        
    except (ValueError, TypeError):
        return False


def print_summary(results: list, args: argparse.Namespace) -> None:
    """æ‰“å°ä¸‹è½½ç»“æœæ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“Š ä¸‹è½½ç»“æœæ‘˜è¦")
    print("="*60)
    
    total_papers = len(results)
    cards_generated = sum(1 for result in results if result.get('has_script_card', False))
    composed_videos = sum(1 for result in results if result.get('composed_video_path'))
    
    print(f"æˆåŠŸå¤„ç†çš„è®ºæ–‡æ•°é‡: {total_papers}")
    print(f"ç”Ÿæˆè§£è¯´å¡ç‰‡æ•°é‡: {cards_generated}")
    print(f"åˆæˆæœ€ç»ˆè§†é¢‘æ•°é‡: {composed_videos}")
    
    if results:
        print(f"è§†é¢‘ä¿å­˜ç›®å½•: {args.download_dir}")
        print(f"å¡ç‰‡ä¿å­˜ç›®å½•: ./cards/")
        print("\nğŸ“ è¯¦ç»†ä¿¡æ¯:")
        
        for i, result in enumerate(results, 1):
            paper = result['paper']
            video = result.get('video')  # ç°åœ¨æ˜¯å•ä¸ªè§†é¢‘è€Œä¸æ˜¯æ•°ç»„
            arxiv_id = result.get('arxiv_id')
            has_card = result.get('has_script_card', False)
            composed_video = result.get('composed_video_path')
            
            print(f"\n{i}. è®ºæ–‡ID: {paper['id']}")
            print(f"   ArXiv ID: {arxiv_id or 'æœªè¯†åˆ«'}")
            print(f"   æ ‡é¢˜: {paper['title'][:80]}...")
            print(f"   ä½œè€…: {', '.join(paper['authors'])}")
            if 'submitted_date' in paper:
                print(f"   æäº¤æ—¥æœŸ: {paper['submitted_date']}")
            
            if video:
                print(f"   è§†é¢‘ç±»å‹: {video.get('video_type', 'æœªçŸ¥')}")
                print(f"   è§£è¯´å¡ç‰‡: {'âœ… å·²ç”Ÿæˆ' if has_card else 'âŒ æœªç”Ÿæˆ'}")
                print(f"   åˆæˆè§†é¢‘: {'âœ… å·²ç”Ÿæˆ' if composed_video else 'âŒ æœªç”Ÿæˆ'}")
                
                if composed_video:
                    composed_filename = os.path.basename(composed_video)
                    print(f"     æœ€ç»ˆè§†é¢‘: {composed_filename}")
                
                filename = os.path.basename(video['local_path'])
                print(f"     ä¸»è§†é¢‘: {filename}")
                print(f"     è§†é¢‘URL: {video['video_url']}")
            else:
                print(f"   è§†é¢‘: âŒ æœªæ‰¾åˆ°")
    else:
        print("\nâŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘")
        print("\nå¯èƒ½çš„åŸå› :")
        print("â€¢ ä»Šå¤©è¯¥é¢†åŸŸæ²¡æœ‰è®ºæ–‡å‘å¸ƒ")
        print("â€¢ è®ºæ–‡æ²¡æœ‰é¡¹ç›®ä¸»é¡µé“¾æ¥")
        print("â€¢ é¡¹ç›®ä¸»é¡µæ²¡æœ‰è§†é¢‘å†…å®¹")
        print("â€¢ ç½‘ç»œè¿æ¥é—®é¢˜")


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå’ŒéªŒè¯å‚æ•°
    args = parse_arguments()
    
    if not validate_arguments(args):
        sys.exit(1)
    
    # è®¾ç½®æ—¥å¿—
    log_level = 10 if args.verbose else 20  # DEBUG if verbose else INFO
    logger = setup_logger('arxiv_crawler', log_level)
    
    # æ‰“å°å¯åŠ¨ä¿¡æ¯
    print("ğŸš€ ArXiv Video Downloader")
    print("="*50)
    print(f"è®ºæ–‡é¢†åŸŸ: {args.field}")
    print(f"æœ€å¤§è®ºæ–‡æ•°: {args.max_papers}")
    print(f"ä¸‹è½½çº¿ç¨‹æ•°: {args.workers}")
    print(f"ä¸‹è½½ç›®å½•: {args.download_dir}")
    if args.cookies_from_browser:
        print(f"Cookieæ¥æº: {args.cookies_from_browser}")
    print("="*50)
    
    logger.info("ArXiv è§†é¢‘ä¸‹è½½å™¨å¯åŠ¨")
    logger.info(f"é…ç½® - é¢†åŸŸ: {args.field}, çº¿ç¨‹æ•°: {args.workers}, "
               f"æœ€å¤§è®ºæ–‡æ•°: {args.max_papers}, ä¸‹è½½ç›®å½•: {args.download_dir}, "
               f"è·³è¿‡å·²å­˜åœ¨è§†é¢‘: {args.skip_existing}, "
               f"Cookieæ¥æº: {args.cookies_from_browser}")
    
    if args.publication_date:
        logger.info(f"ä½¿ç”¨æŒ‡å®šå‘å¸ƒæ—¥æœŸ: {args.publication_date}")
    else:
        logger.info("å°†è‡ªåŠ¨è·å–æœ€æ–°å‘å¸ƒæ—¥æœŸçš„è®ºæ–‡")
    
    crawler = None
    try:
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        crawler = ArxivVideoCrawler(
            download_folder=args.download_dir,
            max_workers=args.workers,
            skip_existing=args.skip_existing,
            cookies_from_browser=args.cookies_from_browser
        )
        
        # å¼€å§‹çˆ¬å–
        results = crawler.crawl_latest_day_videos(
            field=args.field,
            max_papers=args.max_papers,
            target_date=args.publication_date
        )
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print_summary(results, args)
        
        logger.info("ArXiv è§†é¢‘ä¸‹è½½å™¨å®Œæˆ")
        
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        logger.info("ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
        
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}", exc_info=True)
        sys.exit(1)
        
    finally:
        if crawler:
            crawler.close()


if __name__ == "__main__":
    main()
