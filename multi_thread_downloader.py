#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ArXiv Video Downloader - å¤šçº¿ç¨‹ç‰ˆæœ¬
ç®€åŒ–çš„å¯åŠ¨è„šæœ¬ï¼Œæ”¯æŒå¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½

ä½¿ç”¨æ–¹æ³•ï¼š
python multi_thread_downloader.py
"""

import os
import sys
import threading
from datetime import datetime
import argparse  # å¯¼å…¥argparse

# æ·»åŠ crawlerè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'crawler'))
from multi_thread_arxiv_crawler import MultiThreadArxivCrawler

def print_banner():
    """æ‰“å°æ¬¢è¿æ¨ªå¹…"""
    print("ğŸš€ ArXiv Video Downloader - å¤šçº¿ç¨‹ç‰ˆæœ¬")
    print("=" * 60)
    print("âœ¨ ç‰¹æ€§:")
    print("  â€¢ å¤šçº¿ç¨‹å¹¶å‘ä¸‹è½½ï¼Œé€Ÿåº¦æå‡3-5å€")
    print("  â€¢ æ™ºèƒ½ä»»åŠ¡åˆ†å‘ï¼Œé¿å…å•ç‚¹å µå¡")
    print("  â€¢ å®æ—¶è¿›åº¦æ˜¾ç¤ºï¼Œäº†è§£ä¸‹è½½çŠ¶æ€")
    print("  â€¢ tqdmè¿›åº¦æ¡æ˜¾ç¤ºä¸‹è½½è¯¦æƒ…")
    print("  â€¢ æ—¥å¿—è‡ªåŠ¨è®°å½•åˆ°logsæ–‡ä»¶å¤¹")
    print("  â€¢ é˜²åçˆ¬è™«æœºåˆ¶ï¼Œå®‰å…¨ç¨³å®š")
    print("=" * 60)

def get_user_input():
    """è·å–ç”¨æˆ·è¾“å…¥"""
    print("\nğŸ“‹ é…ç½®ä¸‹è½½å‚æ•°:")
    
    # è·å–ä¸‹è½½ç›®å½•
    default_dir = "/Users/zhouzhongtian/Movies/arxiv_video"
    download_dir_input = input(f"ğŸ“ ä¸‹è½½ç›®å½• (é»˜è®¤: {default_dir}): ").strip()
    download_dir = download_dir_input if download_dir_input else default_dir
    print(f"âœ… è®¾ç½®ä¸‹è½½ç›®å½•: {download_dir}")
    
    # è·å–çº¿ç¨‹æ•°
    while True:
        try:
            threads_input = input("ğŸ”§ çº¿ç¨‹æ•°é‡ (å»ºè®®2-8ï¼Œé»˜è®¤4): ").strip()
            max_workers = int(threads_input) if threads_input else 4
            max_workers = max(1, min(max_workers, 16))  # é™åˆ¶åœ¨1-16ä¹‹é—´
            break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    print(f"âœ… è®¾ç½®çº¿ç¨‹æ•°: {max_workers}")
    
    # è·å–ä¸‹è½½æ¨¡å¼
    print("\nğŸ“… é€‰æ‹©ä¸‹è½½æ¨¡å¼:")
    print("1. è·å–æœ€æ–°çš„Nç¯‡è®ºæ–‡")
    print("2. è·å–æŒ‡å®šæ—¥æœŸçš„è®ºæ–‡")
    print("3. è·å–æœ€æ–°å‘å¸ƒæ—¥çš„æ‰€æœ‰è®ºæ–‡")
    
    while True:
        choice = input("è¯·é€‰æ‹©æ¨¡å¼ (1/2/3): ").strip()
        if choice in ['1', '2', '3']:
            break
        print("âŒ è¯·è¾“å…¥ 1ã€2 æˆ– 3")
    
    return max_workers, download_dir, choice

def run_mode_1(crawler):
    """æ¨¡å¼1: æœ€æ–°è®ºæ–‡ - ä½¿ç”¨ç½‘é¡µæŠ“å–"""
    print("\nğŸ”¥ æ¨¡å¼1: ä¸‹è½½æœ€æ–°è®ºæ–‡ (ç½‘é¡µæŠ“å–æ¨¡å¼)")
    
    while True:
        try:
            papers_input = input("ğŸ“„ è¦è·å–çš„è®ºæ–‡æ•°é‡ (å»ºè®®5-50ï¼Œé»˜è®¤25): ").strip()
            if papers_input == '':
                max_papers = 25
                break
            else:
                max_papers = int(papers_input)
                max_papers = max(1, max_papers)
                break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
    
    print(f"âœ… å°†æŠ“å–æœ€æ–°çš„ {max_papers} ç¯‡CS.CVè®ºæ–‡")
    
    # ç›´æ¥è°ƒç”¨æ–°çš„ç½‘é¡µæŠ“å–æ–¹æ³•
    papers = crawler.get_recent_papers(field='cs.CV', max_papers=max_papers)
    
    if not papers:
        print("âŒ æœªèƒ½é€šè¿‡ç½‘é¡µæŠ“å–åˆ°ä»»ä½•è®ºæ–‡ã€‚")
        return []
        
    # ç”±äº get_recent_papers ç›´æ¥è¿”å›è®ºæ–‡ä¿¡æ¯ï¼Œæˆ‘ä»¬éœ€è¦æ‰‹åŠ¨è§¦å‘ä¸‹è½½
    # ä¸ºäº†å¤ç”¨å¤šçº¿ç¨‹ä¸‹è½½é€»è¾‘ï¼Œæˆ‘ä»¬å°†å…¶ä¼ é€’ç»™ä¸€ä¸ªé€šç”¨çš„ä¸‹è½½å¤„ç†å™¨
    return crawler.download_videos_for_papers(papers)

def run_mode_2(crawler):
    """æ¨¡å¼2: æŒ‡å®šæ—¥æœŸè®ºæ–‡"""
    print("\nğŸ“… æ¨¡å¼2: ä¸‹è½½æŒ‡å®šæ—¥æœŸè®ºæ–‡")
    
    while True:
        date_input = input("ğŸ“… è¾“å…¥æ—¥æœŸ (YYYY-MM-DD æˆ– YYYYMMDD): ").strip()
        if len(date_input) in [8, 10]:
            break
        print("âŒ è¯·è¾“å…¥æ­£ç¡®çš„æ—¥æœŸæ ¼å¼")
    
    while True:
        try:
            papers_input = input("ğŸ“„ æœ€å¤§è®ºæ–‡æ•°é‡ (è¾“å…¥'all'ä¸‹è½½å…¨éƒ¨ï¼Œé»˜è®¤100): ").strip()
            
            if papers_input.lower() in ['all', 'å…¨éƒ¨', 'a']:
                max_papers = 999999  # è®¾ç½®è¶³å¤Ÿå¤§çš„æ•°å­—
                print("âœ… å°†ä¸‹è½½è¯¥æ—¥æœŸçš„å…¨éƒ¨CS.CVè®ºæ–‡")
                break
            elif papers_input == '':
                max_papers = 100
                break
            else:
                max_papers = int(papers_input)
                max_papers = max(1, max_papers)  # ç§»é™¤ä¸Šé™é™åˆ¶
                break
        except ValueError:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—æˆ–'all'")
    
    if max_papers != 999999:
        print(f"âœ… å°†ä¸‹è½½ {date_input} çš„ {max_papers} ç¯‡CS.CVè®ºæ–‡")
    
    return crawler.crawl_videos_multi_thread(field='cs.CV', max_papers=max_papers, target_date=date_input)

def run_mode_3(crawler):
    """æ¨¡å¼3: æœ€æ–°å‘å¸ƒæ—¥çš„æ‰€æœ‰è®ºæ–‡"""
    print("\nğŸŒŸ æ¨¡å¼3: ä¸‹è½½æœ€æ–°å‘å¸ƒæ—¥çš„æ‰€æœ‰è®ºæ–‡")
    
    print("ğŸ“„ å°†è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°å‘å¸ƒæ—¥æœŸå¹¶ä¸‹è½½è¯¥æ—¥æœŸçš„æ‰€æœ‰CS.CVè®ºæ–‡")
    
    confirm = input("ç¡®è®¤å¼€å§‹ä¸‹è½½? (y/N): ").strip().lower()
    if confirm not in ['y', 'yes']:
        print("âŒ å–æ¶ˆä¸‹è½½")
        return []
    
    return crawler.crawl_videos_for_latest_day(field='cs.CV', max_papers=1000)

def run_mode_id(crawler, paper_id):
    """æ¨¡å¼ID: ä¸‹è½½æŒ‡å®šIDçš„è®ºæ–‡"""
    print(f"\nğŸ†” æ¨¡å¼ID: ä¸‹è½½è®ºæ–‡ {paper_id}")
    
    # ä½¿ç”¨arxiv APIè·å–å•ä¸ªè®ºæ–‡çš„ä¿¡æ¯
    import arxiv
    try:
        client = arxiv.Client()
        search = arxiv.Search(id_list=[paper_id])
        paper = next(client.results(search), None)
        
        if not paper:
            print(f"âŒ æœªèƒ½æ‰¾åˆ°IDä¸º {paper_id} çš„è®ºæ–‡ã€‚")
            return []
            
        # å°† arxiv.Result å¯¹è±¡è½¬æ¢ä¸ºæˆ‘ä»¬å†…éƒ¨ä½¿ç”¨çš„å­—å…¸æ ¼å¼
        paper_info = {
            'id': paper.entry_id.split('/')[-1],
            'title': paper.title,
            'abstract_url': paper.entry_id,
            'authors': [author.name for author in paper.authors]
        }
        
        # ç›´æ¥è°ƒç”¨å•ä¸ªè®ºæ–‡å¤„ç†å‡½æ•°
        # æ³¨æ„ï¼šprocess_single_paper éœ€è¦ä¸€ä¸ª target_date å‚æ•°ç”¨äºåˆ›å»ºæ–‡ä»¶å¤¹
        # åœ¨è¿™é‡Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è®ºæ–‡çš„å‘å¸ƒæ—¥æœŸ
        target_date = paper.published.strftime("%Y%m%d")
        return [crawler.process_single_paper(paper_info, target_date=target_date)]

    except Exception as e:
        print(f"âŒ è·å–æˆ–å¤„ç†è®ºæ–‡ {paper_id} æ—¶å‡ºé”™: {e}")
        return []

def print_results(results, target_date=None):
    """æ‰“å°ä¸‹è½½ç»“æœ - å¢å¼ºç‰ˆï¼Œå…¼å®¹ä¸åŒæ•°æ®æº"""
    print("\n" + "=" * 60)
    print("ğŸ“Š ä¸‹è½½ç»“æœç»Ÿè®¡")
    print("=" * 60)
    
    if not results:
        print("âŒ æœªæ‰¾åˆ°å¯ä¸‹è½½çš„è§†é¢‘")
        print("\nå¯èƒ½åŸå› :")
        print("  â€¢ è¯¥æ—¶é—´æ®µæ²¡æœ‰CS.CVè®ºæ–‡")
        print("  â€¢ è®ºæ–‡æ²¡æœ‰é¡¹ç›®ä¸»é¡µé“¾æ¥")
        print("  â€¢ é¡¹ç›®ä¸»é¡µæ²¡æœ‰è§†é¢‘å†…å®¹")
        print("  â€¢ ç½‘ç»œè¿æ¥é—®é¢˜")
        return

    successful_papers = [res for res in results if res and res.get('videos')]
    
    print(f"âœ… æˆåŠŸä¸‹è½½äº† {len(successful_papers)} ç¯‡è®ºæ–‡çš„è§†é¢‘")
    
    for result in successful_papers:
        paper_data = result['paper']
        videos = result['videos']
        
        # å…¼å®¹ arxiv.Result å¯¹è±¡å’Œæˆ‘ä»¬è‡ªå·±çš„å­—å…¸
        is_dict = isinstance(paper_data, dict)
        
        paper_id = paper_data['id'] if is_dict else paper_data.entry_id.split('/')[-1]
        title = paper_data['title'] if is_dict else paper_data.title
        authors = paper_data['authors'] if is_dict else [author.name for author in paper_data.authors]
        
        print(f"\nğŸ“„ è®ºæ–‡ID: {paper_id}")
        print(f"   æ ‡é¢˜: {title.strip()}")
        print(f"   ä½œè€…: {', '.join(authors[:3])}")
        print(f"   è§†é¢‘æ•°é‡: {len(videos)}")
        
        for i, video in enumerate(videos):
            local_path = video.get('local_path', 'N/A')
            print(f"     - è§†é¢‘ {i+1}: {os.path.basename(local_path)}")

    # æ‰“å°ä¸‹è½½æ–‡ä»¶å¤¹è·¯å¾„
    if successful_papers:
        # ä»ç¬¬ä¸€ä¸ªç»“æœä¸­è·å–ä¸‹è½½æ–‡ä»¶å¤¹è·¯å¾„
        first_video_path = successful_papers[0]['videos'][0].get('local_path')
        if first_video_path:
            # è·¯å¾„é€šå¸¸æ˜¯ /path/to/download/YYYYMMDD/paper_id/video.mp4
            # æˆ‘ä»¬éœ€è¦è·å–åˆ° /path/to/download
            download_folder = os.path.dirname(os.path.dirname(os.path.dirname(first_video_path)))
            print("\n" + "-" * 60)
            print(f"ğŸ“‚ æ‰€æœ‰è§†é¢‘å·²ä¸‹è½½åˆ°: {download_folder}")
            print("-" * 60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ArXiv Video Downloader - å¤šçº¿ç¨‹ç‰ˆæœ¬")
    parser.add_argument('--workers', type=int, help="çº¿ç¨‹æ•° (1-16)")
    parser.add_argument('--mode', type=str, choices=['latest', 'date', 'latest_day', 'id'], help="è¿è¡Œæ¨¡å¼: 'latest', 'date', 'latest_day', 'id'")
    parser.add_argument('--date', type=str, help="ç›®æ ‡æ—¥æœŸ (å½“æ¨¡å¼ä¸º 'date' æ—¶ä½¿ç”¨, æ ¼å¼: YYYY-MM-DD æˆ– YYYYMMDD)")
    parser.add_argument('--max-papers', type=int, help="æœ€å¤§è®ºæ–‡æ•°")
    parser.add_argument('--paper-id', type=str, help="ç›®æ ‡è®ºæ–‡ID (å½“æ¨¡å¼ä¸º 'id' æ—¶ä½¿ç”¨)")
    parser.add_argument('--download-dir', type=str, help="ä¸‹è½½ç›®å½•è·¯å¾„ (é»˜è®¤: ~/Movies/arxiv_video)")
    
    args = parser.parse_args()

    # å¦‚æœé€šè¿‡å‘½ä»¤è¡Œæä¾›äº†å‚æ•°ï¼Œåˆ™ç›´æ¥è¿è¡Œ
    try:
        if args.mode:  # å¦‚æœé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šäº†æ¨¡å¼ï¼Œåˆ™è¿›å…¥éäº¤äº’æ¨¡å¼
            max_workers = args.workers or 4
            download_dir = args.download_dir or "/Users/zhouzhongtian/Movies/arxiv_video"
            
            print(f"ğŸ”§ åˆå§‹åŒ–å¤šçº¿ç¨‹çˆ¬è™« (çº¿ç¨‹æ•°: {max_workers})...")
            print(f"ğŸ“ ä¸‹è½½ç›®å½•: {download_dir}")
            crawler = MultiThreadArxivCrawler(download_folder=download_dir, max_workers=max_workers)
            
            import time
            start_time = time.time()
            
            print(f"\nğŸš€ å¼€å§‹ä¸‹è½½...")
            print("=" * 60)
            
            results = []
            target_date_for_results = None
            
            if args.mode == 'latest':
                max_papers = args.max_papers if args.max_papers else 25
                papers = crawler.get_recent_papers(field='cs.CV', max_papers=max_papers)
                results = crawler.download_videos_for_papers(papers)
            elif args.mode == 'date':
                if not args.date:
                    print("âŒ é”™è¯¯: ä½¿ç”¨ 'date' æ¨¡å¼æ—¶å¿…é¡»æä¾› --date å‚æ•°ã€‚")
                    return
                max_papers = args.max_papers if args.max_papers else 100
                results = crawler.crawl_videos_multi_thread(field='cs.CV', max_papers=max_papers, target_date=args.date)
                target_date_for_results = args.date
            elif args.mode == 'latest_day':
                max_papers = args.max_papers if args.max_papers else 1000
                results = crawler.crawl_videos_for_latest_day(field='cs.CV', max_papers=max_papers)
                target_date_for_results = "æœ€æ–°å‘å¸ƒæ—¥"
            elif args.mode == 'id':
                if not args.paper_id:
                    print("âŒ é”™è¯¯: ä½¿ç”¨ 'id' æ¨¡å¼æ—¶å¿…é¡»æä¾› --paper-id å‚æ•°ã€‚")
                    return
                results = run_mode_id(crawler, args.paper_id)

            elapsed_time = time.time() - start_time
            print_results(results, target_date_for_results)
            
            print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed_time:.1f} ç§’")
            if results:
                avg_time = elapsed_time / len(results)
                print(f"ğŸ“ˆ å¹³å‡æ¯ç¯‡è®ºæ–‡: {avg_time:.1f} ç§’")
            
            print("\nğŸ‰ ä¸‹è½½å®Œæˆ!")

        else:  # å¦åˆ™ï¼Œä¿æŒåŸæœ‰çš„äº¤äº’æ¨¡å¼
            # æ‰“å°æ¨ªå¹…
            print_banner()
            
            # è·å–ç”¨æˆ·è¾“å…¥
            max_workers, download_dir, choice = get_user_input()
            
            # åˆå§‹åŒ–çˆ¬è™«
            print(f"\nğŸ”§ åˆå§‹åŒ–å¤šçº¿ç¨‹çˆ¬è™« (çº¿ç¨‹æ•°: {max_workers})...")
            print(f"ğŸ“ ä¸‹è½½ç›®å½•: {download_dir}")
            crawler = MultiThreadArxivCrawler(download_folder=download_dir, max_workers=max_workers)
            
            # å¼€å§‹è®¡æ—¶
            import time
            start_time = time.time()
            
            print(f"\nğŸš€ å¼€å§‹ä¸‹è½½...")
            print("=" * 60)
            
            # æ ¹æ®é€‰æ‹©æ‰§è¡Œç›¸åº”æ¨¡å¼
            target_date_for_results = None
            if choice == '1':
                results = run_mode_1(crawler)
            elif choice == '2':
                results = run_mode_2(crawler)
                target_date_for_results = "æŒ‡å®šæ—¥æœŸ"
            elif choice == '3':
                results = run_mode_3(crawler)
                target_date_for_results = "æœ€æ–°å‘å¸ƒæ—¥"
            else:
                results = run_mode_1(crawler)  # é»˜è®¤æ¨¡å¼
            
            # è®¡ç®—è€—æ—¶
            elapsed_time = time.time() - start_time
            
            # æ‰“å°ç»“æœ
            print_results(results, target_date_for_results)
            
            print(f"\nâ±ï¸  æ€»è€—æ—¶: {elapsed_time:.1f} ç§’")
            if results:
                avg_time = elapsed_time / len(results)
                print(f"ğŸ“ˆ å¹³å‡æ¯ç¯‡è®ºæ–‡: {avg_time:.1f} ç§’")
            
            print("\nğŸ‰ ä¸‹è½½å®Œæˆ!")
        
    except KeyboardInterrupt:
        print("\n\nâŒ ç”¨æˆ·ä¸­æ–­ä¸‹è½½")
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
    finally:
        try:
            crawler.close()
        except:
            pass

if __name__ == "__main__":
    main()
